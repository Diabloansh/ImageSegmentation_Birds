# Bird Species Identification with SAM and TensorFlow

This project demonstrates a two-stage computer vision pipeline for detecting, segmenting, and classifying multiple bird species within a single image. It leverages the Segment Anything Model (SAM) for initial object segmentation and a custom-trained TensorFlow/Keras classifier for species identification.

## Project Overview

The primary goal is to accurately place labeled bounding boxes around different birds in a given image. The process is broken down into two main stages:

1.  **Segmentation & Detection:** Instead of traditional object detection, this pipeline uses a segment-then-filter approach. The powerful Segment Anything Model (SAM) generates a large number of potential object masks. These masks are then intelligently filtered to isolate only the birds.
2.  **Classification:** The filtered masks are used to crop the identified bird regions from the original image. Each cropped image is then passed to a pre-trained convolutional neural network (CNN) to classify the specific bird species.

## Key Technologies

*   **Model:** Segment Anything Model (SAM), TensorFlow, Keras
*   **Libraries:** Python, OpenCV, NumPy, Supervision, Matplotlib
*   **Techniques:** Image Segmentation, Color Filtering (HSV), Contour & Shape Analysis, Non-Maximum Suppression (NMS), Deep Learning Classification

## How It Works

The notebook follows a sophisticated pipeline to achieve its goal:

1.  **Initial Segmentation:** The input image is fed into SAM's `SamAutomaticMaskGenerator`, which produces a large set of potential object masks without any prior knowledge of what a "bird" is.

2.  **Intelligent Mask Filtering:** This is the core of the detection stage. A multi-step filtering process is applied to discard irrelevant masks:
    *   **Color Filtering:** The image is converted to the HSV color space, and a broad color mask is created using predefined ranges for common bird colors (e.g., browns, grays, blues, whites).
    *   **Overlap Analysis:** Each SAM mask is compared against the color mask. Only masks with a significant color overlap are kept.
    *   **Shape Analysis:** Contours are extracted from the masks to calculate properties like **area** and **circularity**. Masks that are too large, too small, or have non-bird-like shapes are discarded.

3.  **Bounding Box Generation & Refinement:**
    *   Bounding boxes are generated from the filtered, high-quality masks.
    *   These boxes are further filtered based on their **aspect ratio** and **pixel density** to remove long, thin, or sparse detections.
    *   **Non-Maximum Suppression (NMS)** is applied to the final set of bounding boxes to merge overlapping detections and keep only the most confident one for each bird.

4.  **Classification:**
    *   Each final bounding box is used to crop the bird from the original image.
    *   The cropped image is preprocessed (resized to 224x224, normalized) and fed into a pre-trained Keras model (`my_custom_model.h5`).
    *   The model predicts the species from a list of 25 classes.

5.  **Visualization:** The final output image is generated by drawing the bounding boxes and their predicted class labels on the original image, which is then displayed alongside the input for comparison.

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Install dependencies.** It is recommended to use a virtual environment.
    ```bash
    pip install torch torchvision torchaudio
    pip install opencv-python numpy supervision tensorflow matplotlib
    pip install git+https://github.com/facebookresearch/segment-anything.git
    ```

3.  **Download Model Weights:**
    *   Download the **SAM ViT-H checkpoint** (`sam_vit_h_4b8939.pth`) and place it in the `weights/` directory.
    *   Ensure your pre-trained Keras classifier (`my_custom_model.h5`) is in the project root or update the path in the notebook.

## Usage

1.  Place the image you want to process in the `segmentation/` directory.
2.  Open `FinalSegmentationmodel.ipynb` in a Jupyter environment.
3.  Update the `IMAGE_NAME` variable in cell 4 to match your image file.
4.  Ensure the paths to the SAM checkpoint and Keras model are correct.
5.  Run all cells in the notebook to see the final labeled image.